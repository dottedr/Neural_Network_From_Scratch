{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network w/o NN library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681595a527cd4db8a5501b41725c427f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-images-idx3-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc885c7a59da48a5baf48b4ac5699467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a0762b734c46bda2e16265029c6917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b9c5780b72410eabb2ef5b3b2c6658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sabinaadamska/opt/anaconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /Users/distiller/project/conda/conda-bld/pytorch_1607370126481/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "# Loading code from https://www.youtube.com/watch?v=ixathu7U-LQ&t=1040s\n",
    "# 2 lines\n",
    "train = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "\n",
    "test = datasets.MNIST('', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "\n",
    "# Splitting into data and labels\n",
    "# \n",
    "trainset = train.data.numpy()\n",
    "y_train = train.targets.numpy()\n",
    "\n",
    "testset = test.data.numpy()\n",
    "y_test = test.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function to flatten input\n",
    "from (:,28,28) to (:,784)\n",
    "\"\"\"\n",
    "\n",
    "def reshape_for_nn(orig_dataset):\n",
    "    reshaped_dataset = trainset\n",
    "    reshaped_dataset = reshaped_dataset.reshape(*reshaped_dataset.shape[:1], -1)\n",
    "    return reshaped_dataset\n",
    "    \n",
    "X_train = reshape_for_nn(trainset)\n",
    "X_test = reshape_for_nn(testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper function for one-hot encoding\n",
    "label_range - number of categories\n",
    "label_set - 1D array of target labels\n",
    "\"\"\" \n",
    "\n",
    "def labels_one_hot(label_range,label_set):\n",
    "    label_range = np.arange(10)\n",
    "    label_set = np.array(y_train, ndmin=2).T\n",
    "    one_hot = (label_range==label_set).astype(np.int)\n",
    "    return one_hot\n",
    "\n",
    "one_hot_y_train = labels_one_hot(10,y_train)\n",
    "one_hot_y_test = labels_one_hot(10,y_test)\n",
    "one_hot_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function to normalize the data from range (0,255) to (0,1)\n",
    "Use after calling reshape_for_nn\n",
    "\"\"\"\n",
    "def normalize_data(data):\n",
    "    norm = 1 / 255\n",
    "    norm_data = np.asfarray(data[:]) * norm\n",
    "    return norm_data\n",
    "# Normalizing both train and test\n",
    "X_train = normalize_data(X_train)\n",
    "X_test = normalize_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def nodes(self,nodes):\n",
    "        return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NN2():\n",
    "    def __init__(self,\n",
    "                learning_rate):\n",
    "        layer_nodes = LinearLayer()\n",
    "        self.input_layer = layer_nodes.nodes(784)\n",
    "        self.hidden_layer = layer_nodes.nodes(128)\n",
    "        self.output_layer = layer_nodes.nodes(10)\n",
    "        self.learning_rate =  learning_rate\n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        \n",
    "        self.input_hidden_matrix=np.random.rand(self.hidden_layer,self.input_layer)\n",
    "        self.hidden_output_matrix=np.random.rand(self.output_layer,self.hidden_layer)\n",
    "        return self.input_hidden_matrix, self.hidden_output_matrix\n",
    "    \n",
    "    \n",
    "    def forward(self,input_vector):\n",
    "        # X \n",
    "        self.input_vector=input_vector\n",
    "        # X transposed to match the weights dimensions\n",
    "        self.input_vector = np.array(self.input_vector, ndmin=2).T\n",
    "        # dot product of input and weights \n",
    "        # hidden layer values wrapped in φ activation function\n",
    "        # Transpose was achieved by changing the input order\n",
    "        self.h_values = self.sigmoid(np.dot(self.input_hidden_matrix,self.input_vector))\n",
    "        # ditto for output layer\n",
    "        self.o_values = self.softmax(np.dot(self.hidden_output_matrix,self.h_values))\n",
    "        \n",
    "        return self.o_values\n",
    "        \n",
    "    def sigmoid(self,s):\n",
    "        return 1/(1 + np.exp(-s))\n",
    "    \n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.exp.html\n",
    "    # 1 line    \n",
    "    def softmax(self,x):\n",
    "        e = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return e\n",
    "    \n",
    "    # TODO: dimensions\n",
    "    # Cross entrophy error/loss function\n",
    "    # 1 line form the 7th tutorial\n",
    "    def cross_entropy(self,target, o):\n",
    "        self.entrophy = []\n",
    "        for i in range(len(target)):\n",
    "            e = (-target[i]*np.log(o[i])-(1.0-target[i])*np.log(1.0-o[i]))\n",
    "            self.entrophy.append(e)\n",
    "        return self.entrophy\n",
    "    \n",
    "    # https://www.youtube.com/watch?v=qB2nwJxNVxM&list=PLRqwX-V7Uu6aCibgK1PTWWu9by6XFdCfh&index=17\n",
    "    # the logic follows this video \n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Notes on the solution\n",
    "        gradients = derivative of the output = output *(1-output)\n",
    "        hidden_error = np.dot(self.hidden_output_matrix,self.entrophy)\n",
    "        Δ weights = lr*E*(output*(1-output))H\n",
    "        E is entrophy[i] or other loss function\n",
    "        H h_values or I_values\n",
    "        \"\"\"\n",
    "        self.grad= self.o_values*(1 - self.o_values)\n",
    "        self.placeholder=self.entrophy[1] * self.grad \n",
    "        self.hidden_output_matrix =self.hidden_output_matrix+self.learning_rate*np.dot(self.placeholder,self.h_values.T)\n",
    "        #print(\"matrix\",self.hidden_output_matrix)\n",
    "        \n",
    "        \n",
    "        self.grad_h= self.h_values*(1 - self.h_values)\n",
    "        self.placeholder_h=self.entrophy[0] * self.grad_h\n",
    "        self.input_hidden_matrix =self.input_hidden_matrix+self.learning_rate*np.dot(self.placeholder_h,self.input_vector.T)\n",
    "        #print(\"matrix\",self.hidden_output_matrix)\n",
    "             \n",
    "        return self.hidden_output_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instatiate the network\n",
    "nn2= NN2(0.001) \n",
    "n = 100\n",
    "# training \n",
    "count_correct = 0 \n",
    "total= 0\n",
    "for i in range(60):\n",
    "    #i = 8\n",
    "    X = X_train[i]\n",
    "    y = one_hot_y_train[i]\n",
    "    y_hat=nn2.forward(X)\n",
    "    #loss_function = nn2.simple_error(y,y_hat)\n",
    "    lossf=nn2.cross_entropy(y,y_hat)\n",
    "    updat = nn2.update()\n",
    "\n",
    "    total+=1\n",
    "    if (np.argmax(y_hat) == y_train[i]):\n",
    "        count_correct = count_correct + 1\n",
    "        if (i%n == 0):\n",
    "            break\n",
    "            #print(\"Correct count so far: \", count_correct, \"out of total so far\",total)\n",
    "    break\n",
    "#print(\"Accuracy: {}\".format(count_correct/total))      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other version of calculating the loss and updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_error(self,target,o):\n",
    "        #difference between prediction and labels\n",
    "        self.errors = target - o\n",
    "        return self.errors\n",
    "\n",
    "    def update(self):\n",
    "    # gradients = derivative of the output = output *(1-output)\n",
    "    # hidden_error = np.dot(self.hidden_output_matrix,self.entrophy)\n",
    "    # delta weights_h_o = lr*E *(output*(1-output))H\n",
    "    # E is entrophy[i] or other loss function\n",
    "    # H h_values or I_values\n",
    "    # delta weights_i_h = lr*H_E *(output*(1-output))H\n",
    "    \n",
    "        self.grad= self.o_values*(1 - self.o_values)\n",
    "        #choice of loss function\n",
    "        self.placeholder=self.errors * self.grad \n",
    "        #update the weights\n",
    "        print(\"shape pla\",self.placeholder.shape)\n",
    "        self.hidden_output_delta = self.learning_rate * np.dot(self.placeholder, self.h_values.T)\n",
    "        #print(\"matrix\",self.hidden_output_matrix)\n",
    "        \n",
    "        self.h_error = np.dot(self.hidden_output_matrix.T, output_errors * derivative_output )\n",
    "        \n",
    "        self.grad_h = self.h_values*(1 - self.h_values)\n",
    "        self.placeholder_h = self.h_error * self.grad_h\n",
    "        self.input_hidden_delta = self.learning_rate * np.dot(self.placeholder_h, self.input_vector.T)\n",
    "        #print(\"matrix\",self.hidden_output_matrix)        \n",
    "        \n",
    "        self.hidden_output_matrix += self.hidden_output_delta\n",
    "        self.input_hidden_matrix +=self.input_hidden_delta\n",
    "        \n",
    "        \n",
    "        return self.hidden_output_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
